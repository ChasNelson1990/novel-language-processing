{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Subject Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "from collocater import collocater\n",
    "from spacy_readability import Readability\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_db_url = \"http://corpus-db.org/api\"\n",
    "book = 'The Hound of The Baskervilles'\n",
    "book_id = 3070.0  # have to know the id at the moment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.DataFrame(json.loads(requests.get(corpus_db_url + f\"/id/{book_id}\").text), index=[0])\n",
    "metadata = metadata.replace('', np.nan).dropna(axis=1)\n",
    "\n",
    "display(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Full Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = json.loads(requests.get(corpus_db_url + f\"/id/{book_id}/fulltext\").text)\n",
    "\n",
    "print('Book is {0} characters long.'.format(len(corpus[0]['text'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load into SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_lg  # note this doesn't really work properly with pipenvs\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Add collocater pipeline\n",
    "# collie = collocater.Collocater.loader()  # this adds a lot of processing time for a big text <- can it be sped up?\n",
    "# nlp.add_pipe(collie)\n",
    "\n",
    "# Add readability pipeline\n",
    "nlp.add_pipe(Readability(), last=True)\n",
    "\n",
    "doc_string = re.sub(' +', ' ', corpus[0]['text'].replace('\\r', ' ').replace('\\n', ' ').replace(\"\\'\", \"'\"))  # remove weird characters and extra whitespace\n",
    "doc = nlp(doc_string)  \n",
    "\n",
    "display(doc[:11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of SpaCy properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display([prop for prop in dir(doc) if not prop.startswith('_')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-level Analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Number of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of words is {0}.'.format(len(doc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Words Per Chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chapters(doc):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    doc (spacy.Doc): list of sentences representing a book\n",
    "\n",
    "    Returns:\n",
    "    A list of integers representing the positions of chapters as a word number\n",
    "    \"\"\"\n",
    "    \n",
    "    words = [t.text for t in doc]\n",
    "\n",
    "    chapter_word_idx = [] #represents location of chapters in book\n",
    "    chapter_word_idx.append(0)\n",
    "    for i,w in enumerate(words):\n",
    "        if w == \"Chapter\":\n",
    "            chapter_word_idx.append(i)\n",
    "    chapter_word_idx.append(len(words))\n",
    "        \n",
    "    return np.array(chapter_word_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter_word_idx = split_into_chapters(doc)  # list of chapters locations in words\n",
    "ignore = 16  # ignore the first 16 for tHotB as they're the prelim\n",
    "chapter_word_idx = chapter_word_idx[ignore:]\n",
    "# display(chapter_word_idx)\n",
    "\n",
    "chapters = np.arange(1,len(chapter_word_idx))\n",
    "chapter_word_count = chapter_word_idx[1:] - chapter_word_idx[:-1]\n",
    "# display(chapters, chapter_word_count)\n",
    "\n",
    "plt.figure()\n",
    "sns.barplot(x=chapters, y=chapter_word_count)\n",
    "plt.ylabel('Word Count')\n",
    "plt.xlabel('Chapter')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [w.text.lower() for w in doc]\n",
    "\n",
    "print('Total number of unique words (vocabulary) is {0}.'.format(len(set(words))))\n",
    "\n",
    "chapter_vocab = np.array([len(set(words[chapter_word_idx[i]:chapter_word_idx[i+1]])) for i in np.arange(len(chapters))])\n",
    "\n",
    "plt.figure()\n",
    "sns.barplot(x=chapters, y=100*chapter_vocab/chapter_word_count)\n",
    "plt.ylabel('Unique Word Count/%')\n",
    "plt.ylim(0,100)\n",
    "plt.xlabel('Chapter')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Common Words of Different Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common Nouns\n",
    "nouns = [w.lemma_ for w in doc.noun_chunks if (not w.root.is_stop)]  # this gets rid of common pronouns like 'i', 'he', 'she', etc.\n",
    "nouns = pd.DataFrame(nouns,columns=['word'])\n",
    "nouns = nouns.groupby('word')['word']\n",
    "nouns = nouns.describe()['count']\n",
    "nouns = nouns.sort_values(ascending=False)\n",
    "print(\"The 20 most common nouns are:\")\n",
    "display(nouns.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common Adjectives\n",
    "adjectives = [(w.lemma_) for w in doc if (not (w.is_punct or w.is_space or w.is_stop) and w.pos_==\"ADJ\")]\n",
    "adjectives = pd.DataFrame(adjectives, columns=['word'])\n",
    "adjectives = adjectives.groupby('word')['word']\n",
    "adjectives = adjectives.describe()['count']\n",
    "adjectives = adjectives.sort_values(ascending=False)\n",
    "print(\"The 20 most common adjectives are:\")\n",
    "display(adjectives.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs = [(w.lemma_) for w in doc if (not (w.is_punct or w.is_space or w.is_stop) and w.pos_==\"VERB\")]\n",
    "verbs = pd.DataFrame(verbs, columns=['word'])\n",
    "verbs = verbs.groupby('word')['word']\n",
    "verbs = verbs.describe()['count']\n",
    "verbs = verbs.sort_values(ascending=False)\n",
    "print(\"The 20 most common verbs are:\")\n",
    "display(verbs.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Times\n",
    "# times = [(w.root.text) for w in doc.ents if w.label_ == 'TIME']  # good for showing that most action is at night\n",
    "times = [(w.lemma_) for w in doc.ents if w.label_ == 'TIME']  # shows hours mentioned, e.g. ten o'clock\n",
    "times = pd.DataFrame(times, columns=['word'])\n",
    "times = times.groupby('word')['word']\n",
    "times = times.describe()['count']\n",
    "times = times.sort_values(ascending=False)\n",
    "print(\"The times mentioned are:\")\n",
    "display(times.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persons\n",
    "persons = [(w.lemma_) for w in doc.ents if w.label_ == 'PERSON']\n",
    "persons = pd.DataFrame(persons, columns=['word'])\n",
    "persons = persons.groupby('word')['word']\n",
    "persons = persons.describe()['count']\n",
    "persons = persons.sort_values(ascending=False)\n",
    "print(\"The persons mentioned by name are:\")\n",
    "display(persons.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verbs associated with Holmes\n",
    "holmes_verbs = [(w.sent.root.lemma_) for w in doc if w.text.lower()==\"holmes\"]\n",
    "holmes_verbs = pd.DataFrame(holmes_verbs, columns=['word'])\n",
    "holmes_verbs = holmes_verbs.groupby('word')['word']\n",
    "holmes_verbs = holmes_verbs.describe()['count']\n",
    "holmes_verbs = holmes_verbs.sort_values(ascending=False)\n",
    "print(\"The 20 most common verbs associated with 'Holmes' are:\")\n",
    "display(holmes_verbs.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocations\n",
    "\n",
    "Would like to use `collocater` here but there are some issues for me so have used nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "nltk_doc = nltk.Text(nltk.word_tokenize(doc_string))\n",
    "\n",
    "nltk_doc.collocations(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collocations = pd.DataFrame(data=np.array(doc._.collocs), columns=['collocation'])\n",
    "# collocations = collocations.groupby('collocation')['collocation']\n",
    "# collocations = collocations.describe()['count']\n",
    "# collocations = collocations.sort_values(ascending=False)\n",
    "\n",
    "# display(collocations.head(50).index.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concordances and Dispersion Plots\n",
    "\n",
    "Of words from the book title, key characters and locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_doc.concordance('hound')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_doc.concordance('Stapleton')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "nltk_doc.dispersion_plot(['hound','Hound','Baskervilles','Holmes','Watson','Henry','Stapleton'])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "nltk_doc.dispersion_plot(['Hall', 'moor', 'Coombe', 'Devonshire', 'Grimpen', 'Merripit'])  # note that 'hall' and 'Hall' are different.\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditonal Frequency Distributions\n",
    "\n",
    "Of key characters and locations over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "           (target, c)\n",
    "           for c in chapters\n",
    "           for w in nltk_doc[chapter_word_idx[c-1]:chapter_word_idx[c]]\n",
    "           for target in ['holmes', 'watson', 'henry', 'mortimer', 'stapleton', 'barrymore']\n",
    "           if w.lower().startswith(target))\n",
    "cfd.plot(cumulative=True)\n",
    "plt.xlabel('Chapter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "           (target, c)\n",
    "           for c in chapters\n",
    "           for w in nltk_doc[chapter_word_idx[c-1]:chapter_word_idx[c]]\n",
    "           for target in ['hall', 'moor', 'coombe', 'devonshire', 'grimpen', 'merripit']\n",
    "           if w.lower().startswith(target))\n",
    "cfd.plot(cumulative=True)\n",
    "plt.xlabel('Chapter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Sentence Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_lengths = pd.DataFrame(data=[len(s) for s in doc.sents],columns=['sentence_length']).assign(chapter='all')\n",
    "\n",
    "for c in chapters:\n",
    "    chap = doc[chapter_word_idx[c-1]:chapter_word_idx[c]].as_doc()\n",
    "    sentence_lengths = pd.concat([sentence_lengths, pd.DataFrame(data=[len(s) for s in chap.sents],columns=['sentence_length']).assign(chapter=f'{c:02}')], ignore_index=True)\n",
    "\n",
    "display(sentence_lengths.groupby('chapter').describe().sort_values('chapter'))\n",
    "\n",
    "sns.catplot(data=sentence_lengths, x='chapter', y='sentence_length', kind='violin', width=1, inner=None, cut=0, aspect=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Word Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lengths = pd.DataFrame(data=[len(w) for w in doc],columns=['word_length']).assign(chapter='all')\n",
    "\n",
    "for c in chapters:\n",
    "    chap = doc[chapter_word_idx[c-1]:chapter_word_idx[c]]\n",
    "    word_lengths = pd.concat([word_lengths, pd.DataFrame(data=[len(w) for w in chap],columns=['word_length']).assign(chapter=f'{c:02}')], ignore_index=True)\n",
    "\n",
    "display(word_lengths.groupby('chapter').describe().sort_values('chapter'))\n",
    "\n",
    "sns.catplot(data=word_lengths, x='chapter', y='word_length', kind='violin', width=1, inner=None, cut=0, aspect=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readability Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Flesch-Kincaid Grade Level for the book is {0}'.format(doc._.flesch_kincaid_grade_level))\n",
    "print('Flesch-Kincaid Reading Ease for the book  is {0}'.format(doc._.flesch_kincaid_reading_ease))\n",
    "print('Dale-Chaell for the book  is {0}'.format(doc._.dale_chall))\n",
    "print('SMOG for the book  is {0}'.format(doc._.smog))\n",
    "print('Coleman-Liau Index for the book  is {0}'.format(doc._.coleman_liau_index))\n",
    "print('Automated Readability Index for the book  is {0}'.format(doc._.automated_readability_index))\n",
    "print('FORCAST for the book  is {0}'.format(doc._.forcast))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(index=chapters, columns=['flesch_kincaid_grade_level','flesch_kincaid_reading_ease','dale_chall','smog','coleman_liau_index','automated_readability_index','forcast'], dtype=np.float)\n",
    "\n",
    "for c in chapters:\n",
    "    chap = doc[chapter_word_idx[c-1]:chapter_word_idx[c]].as_doc()\n",
    "    scores.loc[c,'flesch_kincaid_grade_level'] = chap._.flesch_kincaid_grade_level\n",
    "    scores.loc[c,'flesch_kincaid_reading_ease'] = chap._.flesch_kincaid_reading_ease\n",
    "    scores.loc[c,'dale_chall'] = chap._.dale_chall\n",
    "    scores.loc[c,'smog'] = chap._.smog\n",
    "    scores.loc[c,'coleman_liau_index'] = chap._.coleman_liau_index\n",
    "    scores.loc[c,'automated_readability_index'] = chap._.automated_readability_index\n",
    "    scores.loc[c,'forcast'] = chap._.forcast\n",
    "\n",
    "scores = scores.reset_index()\n",
    "scores = scores.rename(columns={\"index\": \"chapter\"})\n",
    "\n",
    "scores = scores.melt(id_vars='chapter',var_name='method',value_name='score')\n",
    "\n",
    "grid = sns.FacetGrid(scores, col=\"method\", col_wrap=4, height=3, sharex=True, sharey=False)\n",
    "grid.map(sns.lineplot, \"chapter\", \"score\")\n",
    "grid.fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ratio of Different POS Across Chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = pd.DataFrame(index=chapters, columns=['ADJ','ADV','INTJ','NOUN','PNOUN','VERB']).fillna(0)\n",
    "\n",
    "for c in chapters:\n",
    "    for w in doc[chapter_word_idx[c-1]:chapter_word_idx[c]]:\n",
    "        if w.pos_ in pos.columns:\n",
    "            pos.loc[c,w.pos_] = pos.loc[c,w.pos_]+1\n",
    "\n",
    "pos = 100*pos.div(pos.sum(axis=1), axis=0)\n",
    "pos.plot(kind='bar', stacked=True)\n",
    "plt.ylabel('% of Words')\n",
    "plt.xlabel('Chapter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title Word Occurence Per Chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_words = ['hound', \"baskerville\"]\n",
    "occurences = pd.DataFrame(index=chapters, columns=title_words).fillna(0)\n",
    "\n",
    "for c in chapters:\n",
    "    for w in doc[chapter_word_idx[c-1]:chapter_word_idx[c]]:\n",
    "        if w.text.lower() in title_words:\n",
    "            occurences.loc[c,w.text.lower()] = occurences.loc[c,w.text.lower()]+1\n",
    "\n",
    "occurences.plot(kind='bar')\n",
    "plt.ylabel('% of Words')\n",
    "plt.xlabel('Chapter')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use of Time Words Throughout Chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time of Day\n",
    "times_of_day = ['dawn','morning','day','noon','afternoon','evening','dusk','night']\n",
    "time_of_day = pd.DataFrame(columns=['chapter','time'])\n",
    "\n",
    "for w in doc.ents:\n",
    "    c = np.digitize(w.start, chapter_word_idx)\n",
    "    if c>0 and w.label_==\"TIME\":\n",
    "        if w.root.lemma_ in times_of_day:\n",
    "            time_of_day = time_of_day.append({'chapter':c,'time':w.root.lemma_}, ignore_index=True)\n",
    "\n",
    "time_of_day = time_of_day.pivot_table(index='chapter',columns='time', aggfunc=len).reset_index().melt(id_vars='chapter',var_name='word',value_name='count')\n",
    "\n",
    "g = sns.catplot(data=time_of_day, x='chapter', y='count', kind='bar', col='word', col_order=times_of_day, col_wrap=3, height=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hour of Day\n",
    "numbers = [\"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"eleven\", \"twelve\"]\n",
    "hours = [w + \" o'clock\" for w in numbers]\n",
    "numeric_hours = [f\"{w} o'clock\" for w in np.arange(1,13)]\n",
    "\n",
    "hour_of_day = pd.DataFrame(columns=['chapter','time'])\n",
    "\n",
    "for w in doc.ents:\n",
    "    c = np.digitize(w.start, chapter_word_idx)\n",
    "    if c>0 and w.label_==\"TIME\":\n",
    "        for (h,n) in zip(hours,numeric_hours):\n",
    "            if h in w.text or n in w.text:\n",
    "                hour_of_day = hour_of_day.append({'chapter':c,'time':h}, ignore_index=True)\n",
    "\n",
    "hour_of_day = hour_of_day.pivot_table(index='chapter',columns='time', aggfunc=len).reset_index().melt(id_vars='chapter',var_name='word',value_name='count')\n",
    "\n",
    "g = sns.catplot(data=hour_of_day, x='chapter', y='count', kind='bar', col='word', col_order=hours, col_wrap=3, height=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use SpaCy's Similarity To Investigate Chapter Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = np.zeros((len(chapters),len(chapters)))\n",
    "\n",
    "for c1 in chapters:\n",
    "    chap1 = doc[chapter_word_idx[c1-1]:chapter_word_idx[c1]].as_doc()\n",
    "    for c2 in chapters: \n",
    "        chap2 = doc[chapter_word_idx[c2-1]:chapter_word_idx[c2]].as_doc()\n",
    "        similarity_matrix[c1-1,c2-1] = chap1.similarity(chap2)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(similarity_matrix)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "novel-language-processing",
   "language": "python",
   "name": "novel-language-processing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
